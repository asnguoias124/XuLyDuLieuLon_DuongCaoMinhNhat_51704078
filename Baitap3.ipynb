{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baitap3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPPNPapdhXGkikDQ4Kso+pF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asnguoias124/XuLyDuLieuLon_DuongCaoMinhNhat_51704078/blob/main/Baitap3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00AR7NNpFi2y",
        "outputId": "2a73371a-db7f-4291-d94a-03402c89b2a5"
      },
      "source": [
        "!pip install pyspark\r\n",
        "import pyspark\r\n",
        "\r\n",
        "from pyspark import SparkConf, SparkContext\r\n",
        "import collections\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from google.colab import drive\r\n",
        "import collections\r\n",
        "\r\n",
        "#Read File\r\n",
        "file =  sc.textFile(\"store_data.csv\")\r\n",
        "## Splited items  \r\n",
        "lblitems = file.map(lambda line: line.split(','))\r\n",
        "\r\n",
        "#print(lblitems.collect())\r\n",
        "## Whole lines in single array \r\n",
        "wlitems = file.flatMap(lambda line:line.split(','))\r\n",
        "\r\n",
        "#print(wlitems.collect())\r\n",
        "## Unique frequent items in dataset\r\n",
        "uniqueItems = wlitems.distinct()\r\n",
        "\r\n",
        "# Add 1 as Tuple\r\n",
        "supportRdd = wlitems.map(lambda item: (item , 1))\r\n",
        "\r\n",
        "# Method for sum in reduceByKey method\r\n",
        "def sumOparator(x,y):\r\n",
        "    return x+y\r\n",
        "\r\n",
        "# Sum of values by key\r\n",
        "supportRdd = supportRdd.reduceByKey(sumOparator)\r\n",
        "\r\n",
        "# print(supportRdd.collect()) # Retruns following array \r\n",
        "# [('Apple', 12), ('Mango', 10), ('Banana', 9), ('Coconut', 3), \r\n",
        "#  ('Strawberry', 4), ('Grapes', 2), ('Lemon', 1), ('Raspberry', 9), ('Rassberry', 1)]\r\n",
        "\r\n",
        "\r\n",
        "# First support values\r\n",
        "supports = supportRdd.map(lambda item: item[1]) # Return only support values\r\n",
        "\r\n",
        "#Use supportRdd.collect() to  get the first item -support tuples (assume as Table A)\r\n",
        "\r\n",
        "# Define minimum support value \r\n",
        "minSupport = supports.min()\r\n",
        "\r\n",
        "# If mininmum support is 1 then replace it with 325 \r\n",
        "minSupport = 325 if minSupport == 1 else minSupport\r\n",
        "\r\n",
        "## Filter first supportRdd with minimum support \r\n",
        "supportRdd = supportRdd.filter(lambda item: item[1] >= minSupport )\r\n",
        "\r\n",
        "## Craete base RDD with will be updated every iteration\r\n",
        "baseRdd = supportRdd.map(lambda item: ([item[0]] , item[1])) \r\n",
        "print('1 . Table has crated...') \r\n",
        "\r\n",
        "supportRdd = supportRdd.map(lambda item: item[0])\r\n",
        "supportRddCart = supportRdd\r\n",
        "\r\n",
        "def removeReplica(record):\r\n",
        "\r\n",
        "    if(isinstance(record[0], tuple)):\r\n",
        "        x1 = record[0]\r\n",
        "        x2 = record[1]\r\n",
        "    else:\r\n",
        "        x1 = [record[0]]\r\n",
        "        x2 = record[1]\r\n",
        "\r\n",
        "    if(any(x == x2 for x in x1) == False):\r\n",
        "        a = list(x1)\r\n",
        "        a.append(x2)\r\n",
        "        a.sort()\r\n",
        "        result = tuple(a)\r\n",
        "        return result \r\n",
        "    else:\r\n",
        "        return x1\r\n",
        "\r\n",
        "c = 2 # Combination length \r\n",
        "\r\n",
        "while(supportRdd.isEmpty() == False):\r\n",
        "\r\n",
        "    combined = supportRdd.cartesian(uniqueItems)\r\n",
        "    combined = combined.map(lambda item: removeReplica(item))\r\n",
        "  \r\n",
        "    combined = combined.filter(lambda item: len(item) == c)\r\n",
        "    combined = combined.distinct()\r\n",
        "\r\n",
        "    \r\n",
        "    combined_2 = combined.cartesian(lblitems)\r\n",
        "    combined_2 = combined_2.filter(lambda item: all(x in item[1] for x in item[0]))\r\n",
        "    \r\n",
        "    combined_2 = combined_2.map(lambda item: item[0])\r\n",
        "    combined_2 = combined_2.map(lambda item: (item , 1))\r\n",
        "    combined_2 = combined_2.reduceByKey(sumOparator)\r\n",
        "    combined_2 = combined_2.filter(lambda item: item[1] >= minSupport)\r\n",
        "\r\n",
        "    baseRdd = baseRdd.union(combined_2)\r\n",
        "    \r\n",
        "    combined_2 = combined_2.map(lambda item: item[0])\r\n",
        "    supportRdd = combined_2\r\n",
        "    print(c ,'. Table has crated... ')\r\n",
        "    c = c+1 \r\n",
        "\r\n",
        "class Filter():\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        \r\n",
        "        self.stages = 1\r\n",
        "\r\n",
        "\r\n",
        "    def filterForConf(self, item , total):\r\n",
        "        \r\n",
        "        if(len(item[0][0]) > len(item[1][0])  ):\r\n",
        "            if(self.checkItemSets(item[0][0] , item[1][0]) == False):\r\n",
        "                pass\r\n",
        "            else:\r\n",
        "                return (item)       \r\n",
        "        else:\r\n",
        "            pass  \r\n",
        "        self.stages = self.stages + 1\r\n",
        "\r\n",
        "    # Check Items sets includes at least one comman item // Example command: # any(l == k for k in z for l in x )\r\n",
        "    def checkItemSets(self, item_1 , item_2):\r\n",
        "\r\n",
        "        if(len(item_1) > len(item_2)):\r\n",
        "            return all(any(k == l for k in item_1 ) for l in item_2)\r\n",
        "        else:\r\n",
        "            return all(any(k == l for k in item_2 ) for l in item_1)\r\n",
        "\r\n",
        "\r\n",
        "    def calculateConfidence(self, item):\r\n",
        "\r\n",
        "        # Parent item list\r\n",
        "        parent = set(item[0][0])\r\n",
        "        \r\n",
        "        # Child item list\r\n",
        "        if(isinstance(item[1][0] , str)):\r\n",
        "            child  = set([item[1][0]])\r\n",
        "        else:\r\n",
        "            child  = set(item[1][0])\r\n",
        "        # Parent and Child support values\r\n",
        "        parentSupport = item[0][1]\r\n",
        "        childSupport = item[1][1]\r\n",
        "        # Finds the item set confidence is going to be found\r\n",
        "\r\n",
        "        support = (parentSupport / childSupport)*100\r\n",
        "\r\n",
        "        return list([ list(child) ,  list(parent.difference(child)) , support ])\r\n",
        "\r\n",
        "        \r\n",
        "# Example ((('x10', 'x3', 'x6', 'x7', 'x9'), 1), (('x10', 'x3', 'x7'), 1))\r\n",
        "calcuItems = baseRdd.cartesian(baseRdd)\r\n",
        "\r\n",
        "# Create Filter Object\r\n",
        "ff = Filter()\r\n",
        "\r\n",
        "#deneme = calcuItems.map(lambda item: lens(item)) \r\n",
        "total = calcuItems.count()\r\n",
        "\r\n",
        "print('# : Aggregated support values preparing for the confidence calculatations')\r\n",
        "baseRddConfidence = calcuItems.filter(lambda item: ff.filterForConf(item , total))\r\n",
        "print('# : Aggregated support values are ready !')\r\n",
        "baseRddConfidence = baseRddConfidence.map(lambda item: ff.calculateConfidence(item))\r\n",
        "\r\n",
        "  \r\n",
        "print(baseRddConfidence.collect())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "1 . Table has crated...\n",
            "2 . Table has crated... \n",
            "3 . Table has crated... \n",
            "# : Aggregated support values preparing for the confidence calculatations\n",
            "# : Aggregated support values are ready !\n",
            "[[['mineral water'], ['milk'], 20.13422818791946], [['milk'], ['mineral water'], 37.03703703703704], [['mineral water'], ['eggs'], 21.364653243847876], [['mineral water'], ['spaghetti'], 25.05592841163311], [['eggs'], ['mineral water'], 28.338278931750743], [['spaghetti'], ['mineral water'], 34.30321592649311], [['mineral water'], ['chocolate'], 22.091722595078302], [['chocolate'], ['mineral water'], 32.113821138211385]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}